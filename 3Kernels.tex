% -*- root: Main.tex -*-
\section*{Kernels}
\subsection*{Reformulating the perceptron}
Ansatz: $w=\sum_{j=1}^n \alpha_j y_j x_j$\\
$\min \limits_{w\in\mathbb{R}^d} \sum_{i=1}^n \max [0, -y_i w^T x_i]$\\
$= \min \limits_{\alpha_{1:n}} \sum_{i=1}^n \max [0,-y_i  ( \sum_{j=1}^n \alpha_j y_j x_j  )^T x_i ]$\\
$= \min \limits_{\alpha_{1:n}} \sum_{i=1}^n \max  [0,- \sum_{j=1}^n \alpha_j y_i y_j x_i^T x_j ]$

\subsection*{Polynomial kernel}
$k_1 = (x^Ty)^m$ represents monomial of deg m \\
$k_2 = (1+x^Ty)^m$ represents monomials up to deg m

\subsection*{Kernelized Perceptron}
1. Initialize $\alpha_1 = ... = \alpha_n = 0$\\
2. For t do \\
Pick data $(x_i,y_i) \in_{u.a.r} D$\\
Predict $\hat{y} = sign(\sum_{j=1}^n \alpha_j y_j k(x_j,x_i))$\\
If $\hat{y} \not = y_i$ set $\alpha_i = \alpha_i + \eta_t$


\subsection*{Properties of kernel}
\begin{itemize}
	\item k must be symmetric
	\item the kernel matrix must be SPD
\end{itemize}

\subsection*{Kernel matrix}
The kernel matrix $K$ is SPD \\
$K = 
\begin{bmatrix}
	k(x_1,x_1) & \dots & k(x_1,x_n) \\
	\vdots & \ddots & \vdots \\
	k(x_n, x_1) & \dots & k(x_n,x_n)
\end{bmatrix}$\\
$\left ( XX^T \right )$ for inner product as kernel.

\subsection*{semi-positive-definite matrices}
$M \in \mathbb{R}^{n\times n}$ is SPD $\Leftrightarrow$\\
$\forall x \in \mathbb{R}^n: x^TMx \geq 0 \Leftrightarrow$\\
all eigenvalues of $M$ are positive $\geq 0$

%\subsection*{Nearest Neighbor k-NN}
%$y=sign(\sum \limits_{i=1}^n y_i [x_i \text{ among k nearest neighbors of } x])$
%
\subsection*{Kernel engineering}
$k_1(x,y) + k_2(x,y)$\\
$k_1(x,y) \cdot k_2(x,y)$\\
$c \cdot k_1(x,y)$ for $c>0$\\
$f(k_1(x,y))$, where $f$ is exponential/polynomial with positive coefficents

\subsection*{Parametric vs. Nonparametric}
\emph{Parametric}: have finite set of parameters\\
E.g. linear regression, perceptron,...\\
$f(x) = w^Tx, w\in \mathbb{R}^d$ (d is independent of #data)
% \begin{itemize}
% 	\item[+] computationally not complex
% \end{itemize}

\emph{Nonparametric}: grows in complexity with the size of the data\\
E.g. kernelized Perceptron, k-NN,...\\
$f(x) = \sum_{i=1}^n \alpha_i y_i k(x_i,x_n)$ (depends on #data)\\
% \begin{itemize}
% 	\item[+] potentially much more expressive.
% \end{itemize}

\subsection*{Parametric to nonparametric linear regression}
Ansatz: $w=\sum_i \alpha_i x$\\
Parametric: $w^* = \underset{w}{\operatorname{argmin}} \sum_i (w^Tx_i-y_i)^2 + \lambda ||w||_2^2$\\
$= \underset{\alpha_{1:n}}{\operatorname{argmin}} \sum \limits_{i=1}^n (\sum \limits_{j=1}^n \alpha_j x_j^T x_i - y_i)^2 + \lambda \sum \limits_i \sum \limits_j \alpha_i \alpha_j (x_i^T x_j)$\\
$= \underset{\alpha_{1:n}}{\operatorname{argmin}} \sum \limits_{i=1}^n (\alpha^T K_i - y_i)^2 + \lambda \alpha^T K \alpha$\\
$= \underset{\alpha}{\operatorname{argmin}} ||\alpha^T K -y||_2^2 + \lambda \alpha^T K \alpha$\\
Closed form: $\alpha^* = (K+\lambda I)^{-1} y$\\
Prediction: $y^*= w^{*T} x = \sum \limits_{i=1}^n \alpha_i^* k(x_i,x)$

\section*{Feature Selection}
\subsection*{Lasso regression}
$w^* = \underset{w}{\operatorname{argmin}} \sum \limits_{i=1}^n (y_i - w^Tx_i)^2 + \lambda ||w||_1$\\
The alternative penalty encourages coefficients to be exactly 0 (automatic feature selection).

\subsection*{Sparse L1-SVM}
$\underset{w}{\operatorname{argmin}} \sum \limits_{i=1}^n \max (0, 1-y_i w^T x_i) + \lambda ||w||_1$

\subsection*{PCA}
$\Sigma = \frac{1}{n}\sum_{i=1}^n x_i*x_i^T$
solution to PCA problem $(W,z_i)=\operatorname{argmin}\sum_{i=1}^n ||Wz_i-x_i||_2^2$ ($W\in \mathbb{R}^{d \times k}, z_i\in \mathbb{R}^k$) is $W = (v_1|...|v_k)$, $v_i$ ev. of $\Sigma$ and $z_i = W^Tx_i$